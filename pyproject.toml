[project]
name = "bpmextension"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = "==3.10.*"
dependencies = [
    "polars==1.20.0",
    "python-statemachine==2.5.0",
    "llama-cpp-python==0.3.*",
    "torch==2.5.1",
    "torchvision==0.20.1",
    "accelerate>=1.3.0",
    "auto-gptq>=0.7.1",
    "jax>=0.5.0",
    "langchain>=0.3.16",
    "langchain-community>=0.3.16",
    "qwen-vl-utils>=0.0.10",
    "optimum>=1.23.3",
    "transformers>=4.48.1",
    "sqlite-vec>=0.1.6",
]

[dependency-groups]
dev = ["setuptools>=70.0.0", "pre-commit==4.0.1"]

[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages]
find = {}

[[tool.uv.index]]
name = "torch-gpu"
url = "https://download.pytorch.org/whl/cu124"

[[tool.uv.index]]
name = "llama-cpp-python-gpu"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124"

[project.scripts]
dev-setup = "scripts.dev_setup:main"

[tool.ruff]

fix = true
show-fixes = true
include = ["pyproject.toml", "src/**/*.py", "src/**/*.ipynb", "scripts/**/*.py"]
exclude = ["src/semantics/prompts.py"]


[tool.ruff.lint]

ignore = ["F841", "E203", "E501"]

[tool.ruff.format]

docstring-code-format = true

[tool.mypy]

ignore_missing_imports = true
